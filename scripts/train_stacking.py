"""
Train stacking meta-model for 1X2 predictions.

Usage:
    python scripts/train_stacking.py [--league-id 39] [--min-samples 100] [--c 1.0]
    python scripts/train_stacking.py --from-file results/training_data.json [--dry-run]

Reads settled predictions from DB or from generated JSON file,
extracts base model probs + features,
trains LogisticRegression meta-model, saves coefficients to model_params.

Walk-forward safety: uses predictions already made BEFORE each match
(stored in feature_flags by build_predictions.py). This is Variant A (simple OOS).
--from-file uses walk-forward generated data from generate_training_data.py.
"""
from __future__ import annotations

import argparse
import asyncio
import json
import sys
from decimal import Decimal
from pathlib import Path

import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from app.core.db import SessionLocal, init_db  # noqa: E402
from app.core.logger import get_logger  # noqa: E402
from app.services.metrics import ranked_probability_score, brier_score, log_loss_score  # noqa: E402
from app.core.decimalutils import D  # noqa: E402

log = get_logger("scripts.train_stacking")

# Feature names expected in feature_flags (written by build_predictions.py)
# v2: 13 features — added DC-xG (3), removed standings_delta (dead feature in hist_fixtures)
STACKING_FEATURE_NAMES = [
    "p_home_poisson",
    "p_draw_poisson",
    "p_away_poisson",
    "p_home_dc",
    "p_draw_dc",
    "p_away_dc",
    "p_home_dc_xg",
    "p_draw_dc_xg",
    "p_away_dc_xg",
    "elo_diff",
    "fair_home",
    "fair_draw",
    "fair_away",
]


async def load_training_data(session, league_id: int | None, min_samples: int):
    """Load settled predictions with base model probabilities from feature_flags."""
    from sqlalchemy import text

    league_filter = ""
    params: dict = {}
    if league_id is not None:
        league_filter = "AND f.league_id = :lid"
        params["lid"] = league_id

    res = await session.execute(
        text(
            f"""
            SELECT
                p.fixture_id,
                p.feature_flags,
                p.status,
                f.home_goals,
                f.away_goals,
                f.league_id
            FROM predictions p
            JOIN fixtures f ON f.id = p.fixture_id
            WHERE p.status IN ('WIN', 'LOSS')
              AND p.selection_code != 'SKIP'
              AND p.feature_flags IS NOT NULL
              AND f.home_goals IS NOT NULL
              AND f.away_goals IS NOT NULL
              {league_filter}
            ORDER BY f.kickoff ASC
            """
        ),
        params,
    )
    rows = res.fetchall()

    features_list = []
    labels_list = []
    skipped = 0

    for row in rows:
        flags = row.feature_flags if isinstance(row.feature_flags, dict) else {}

        # Check that base model probs are available
        has_poisson = "p_home_poisson" in flags

        if not has_poisson:
            skipped += 1
            continue

        # Fallback: if DC-xG is missing, use DC-goals values
        if flags.get("p_home_dc_xg") is None:
            flags["p_home_dc_xg"] = flags.get("p_home_dc")
            flags["p_draw_dc_xg"] = flags.get("p_draw_dc")
            flags["p_away_dc_xg"] = flags.get("p_away_dc")

        # Build feature vector
        fv = []
        for fname in STACKING_FEATURE_NAMES:
            val = flags.get(fname)
            if val is None:
                val = 0.0
            fv.append(float(val))
        features_list.append(fv)

        # Label: 0=home, 1=draw, 2=away
        hg, ag = int(row.home_goals), int(row.away_goals)
        if hg > ag:
            labels_list.append(0)
        elif hg == ag:
            labels_list.append(1)
        else:
            labels_list.append(2)

    log.info(
        "training_data loaded=%d skipped=%d (missing base probs)",
        len(features_list),
        skipped,
    )

    if len(features_list) < min_samples:
        log.warning(
            "insufficient samples: %d < %d (min_samples). "
            "Run build_predictions with base model probs in feature_flags first.",
            len(features_list),
            min_samples,
        )
        return None, None

    return np.array(features_list), np.array(labels_list)


def load_training_data_from_file(filepath: str, league_id: int | None, min_samples: int):
    """Load training data from JSON file generated by generate_training_data.py."""
    with open(filepath) as f:
        payload = json.load(f)

    data = payload.get("data", [])
    if league_id is not None:
        data = [d for d in data if d.get("league_id") == league_id]

    features_list = []
    labels_list = []
    skipped = 0

    for row in data:
        # Require at least Poisson probs
        if row.get("p_home_poisson") is None:
            skipped += 1
            continue

        # Fallback: if DC-xG is missing, use DC-goals values (better than 0.0)
        if row.get("p_home_dc_xg") is None:
            row["p_home_dc_xg"] = row.get("p_home_dc")
            row["p_draw_dc_xg"] = row.get("p_draw_dc")
            row["p_away_dc_xg"] = row.get("p_away_dc")

        fv = []
        for fname in STACKING_FEATURE_NAMES:
            val = row.get(fname)
            if val is None:
                val = 0.0
            fv.append(float(val))
        features_list.append(fv)
        labels_list.append(int(row["outcome"]))

    log.info(
        "from_file loaded=%d skipped=%d (missing base probs) league=%s",
        len(features_list), skipped, league_id or "all",
    )

    if len(features_list) < min_samples:
        log.warning("insufficient samples: %d < %d", len(features_list), min_samples)
        return None, None

    return np.array(features_list), np.array(labels_list)


async def save_stacking_params(session, model, league_id: int | None, n_train: int, metrics: dict):
    """Save trained model to DB via stacking service."""
    from app.services.stacking import save_stacking_model

    await save_stacking_model(
        session,
        coefficients=model.coef_,
        intercept=model.intercept_,
        feature_names=STACKING_FEATURE_NAMES,
        league_id=league_id,
        n_samples=n_train,
        val_rps=metrics.get("rps", 0.0),
        val_logloss=metrics.get("logloss", 0.0),
    )
    await session.commit()


def evaluate_predictions(probs: np.ndarray, labels: np.ndarray) -> dict:
    """Compute RPS, Brier, LogLoss on validation set."""
    n = len(labels)
    if n == 0:
        return {"rps": 0.0, "brier": 0.0, "logloss": 0.0, "n": 0}

    rps_sum = 0.0
    brier_sum = 0.0
    logloss_sum = 0.0

    for i in range(n):
        p_home = D(str(round(probs[i, 0], 6)))
        p_draw = D(str(round(probs[i, 1], 6)))
        p_away = D(str(round(probs[i, 2], 6)))
        outcome_idx = int(labels[i])

        rps_sum += float(ranked_probability_score((p_home, p_draw, p_away), outcome_idx))

        # Brier: per-class average
        for cls in range(3):
            outcome = 1 if cls == outcome_idx else 0
            brier_sum += float(brier_score(D(str(round(probs[i, cls], 6))), outcome))

        # LogLoss: on predicted prob for actual class
        p_actual = max(probs[i, outcome_idx], 1e-15)
        logloss_sum += -np.log(p_actual)

    return {
        "rps": rps_sum / n,
        "brier": brier_sum / (n * 3),
        "logloss": logloss_sum / n,
        "n": n,
    }


async def main(args):
    # 1. Load data
    if args.from_file:
        features, labels = load_training_data_from_file(
            args.from_file, args.league_id, args.min_samples
        )
    else:
        await init_db()
        async with SessionLocal() as session:
            features, labels = await load_training_data(session, args.league_id, args.min_samples)

    if features is None:
        log.error("Not enough data to train. Exiting.")
        return

    log.info("Dataset: %d samples, %d features", len(features), features.shape[1])

    # 2. Chronological split (NOT random!)
    split_idx = int(len(features) * 0.8)
    X_train, X_val = features[:split_idx], features[split_idx:]
    y_train, y_val = labels[:split_idx], labels[split_idx:]

    log.info("Split: train=%d val=%d", len(X_train), len(X_val))

    if len(X_train) < 30:
        log.error("Training set too small (%d). Need at least 30.", len(X_train))
        return

    # 3. Train
    try:
        from sklearn.linear_model import LogisticRegression
    except ImportError:
        log.error("scikit-learn not installed. Run: pip install scikit-learn>=1.4.0")
        return

    model = LogisticRegression(
        C=args.c,
        penalty="l2",
        max_iter=1000,
        solver="lbfgs",
    )
    model.fit(X_train, y_train)
    log.info("Model trained. Classes: %s", model.classes_.tolist())
    log.info("Coefficients shape: %s", model.coef_.shape)

    # 4. Evaluate on validation
    probs_val = model.predict_proba(X_val)
    metrics_val = evaluate_predictions(probs_val, y_val)
    log.info(
        "Validation: RPS=%.4f Brier=%.4f LogLoss=%.4f n=%d",
        metrics_val["rps"],
        metrics_val["brier"],
        metrics_val["logloss"],
        metrics_val["n"],
    )

    # Also evaluate on train for comparison
    probs_train = model.predict_proba(X_train)
    metrics_train = evaluate_predictions(probs_train, y_train)
    log.info(
        "Train:      RPS=%.4f Brier=%.4f LogLoss=%.4f n=%d",
        metrics_train["rps"],
        metrics_train["brier"],
        metrics_train["logloss"],
        metrics_train["n"],
    )

    # 5. Print feature importance
    print("\n=== Feature Importance ===")
    for i, name in enumerate(STACKING_FEATURE_NAMES):
        coefs = model.coef_[:, i]
        print(f"  {name:25s}  H={coefs[0]:+.4f}  D={coefs[1]:+.4f}  A={coefs[2]:+.4f}")

    print(f"\nIntercept:  H={model.intercept_[0]:+.4f}  D={model.intercept_[1]:+.4f}  A={model.intercept_[2]:+.4f}")

    # 6. Save coefficients
    if args.dry_run:
        log.info("Dry run — not saving to DB.")
    else:
        if args.from_file:
            await init_db()
        async with SessionLocal() as session:
            await save_stacking_params(
                session,
                model,
                league_id=args.league_id,
                n_train=len(X_train),
                metrics=metrics_val,
            )
            log.info("Model saved to model_params (scope='stacking').")

    # 7. Print summary
    print("\n=== Summary ===")
    print(f"Training samples:   {len(X_train)}")
    print(f"Validation samples: {len(X_val)}")
    print(f"Validation RPS:     {metrics_val['rps']:.4f}")
    print(f"Validation LogLoss: {metrics_val['logloss']:.4f}")
    print(f"Validation Brier:   {metrics_val['brier']:.4f}")
    print(f"League ID:          {args.league_id or 'global'}")
    print(f"Regularization C:   {args.c}")
    print(f"Source:             {'file: ' + args.from_file if args.from_file else 'database'}")


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train stacking meta-model for 1X2 predictions"
    )
    parser.add_argument(
        "--from-file",
        type=str,
        default=None,
        help="Load training data from JSON file (from generate_training_data.py)",
    )
    parser.add_argument(
        "--league-id",
        type=int,
        default=None,
        help="Train per-league model (default: global across all leagues)",
    )
    parser.add_argument(
        "--min-samples",
        type=int,
        default=100,
        help="Minimum settled predictions required (default: 100)",
    )
    parser.add_argument(
        "--c",
        type=float,
        default=1.0,
        help="Regularization parameter C for LogisticRegression (default: 1.0)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Train and evaluate but don't save to DB",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    asyncio.run(main(args))
